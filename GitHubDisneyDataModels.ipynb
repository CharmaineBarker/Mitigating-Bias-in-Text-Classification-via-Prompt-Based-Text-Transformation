{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Using the Disneyland Dataset available from kaggle at:\n",
        "https://www.kaggle.com/datasets/arushchillar/disneyland-reviews"
      ],
      "metadata": {
        "id": "IqxRGSzO9Wrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How to load the dictionaries\n",
        "#read_dictionary = np.load('nbDict.npy', allow_pickle='TRUE').item()"
      ],
      "metadata": {
        "id": "6AgkqLFdBlZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialisations"
      ],
      "metadata": {
        "id": "fJ3V0MEqRSHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from tqdm import trange\n",
        "from nltk import tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.probability import FreqDist\n",
        "from collections import Counter\n",
        "from textblob import TextBlob\n",
        "\n",
        "# bag of words\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#for model-building\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.metrics import *"
      ],
      "metadata": {
        "id": "zMzGQfZ2SEKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "sns.set_style('darkgrid')\n",
        "plt.rcParams['figure.figsize'] = (17,7)\n",
        "plt.rcParams['font.size'] = 12\n",
        "sns.set_palette(\"Paired\")"
      ],
      "metadata": {
        "id": "hXM7wLSPSGXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "B-stO1OYpyZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url='https://drive.google.com/file/d/1BEkeqcbJtVcgWbawPxJVVV2kmwRhYbTV/view?usp=sharing'\n",
        "url='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
        "data = pd.read_csv(url)\n",
        "data.head(1)"
      ],
      "metadata": {
        "id": "ygCJu8uBRaa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.groupby('reviewerLocation', as_index=False).apply(lambda x: x.sample(739, random_state=64))\n",
        "data = data.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Lf-CCljDg1jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Change the labels to numeric values\n",
        "# 0 - Australia, 1 - United Kingdom, 2 - United States\n",
        "labelEncoder = LabelEncoder()\n",
        "data.reviewerLocation = labelEncoder.fit_transform(data.reviewerLocation)"
      ],
      "metadata": {
        "id": "LkqSQJNutzXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Text"
      ],
      "metadata": {
        "id": "Q4ASPY-SuYfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing pipeline from https://www.kaggle.com/code/balatmak/text-preprocessing-steps-and-universal-pipeline\n",
        "import numpy as np\n",
        "import multiprocessing as mp\n",
        "\n",
        "import string\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "from sklearn import preprocessing\n",
        "\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self,\n",
        "                 variety=\"BrE\",\n",
        "                 user_abbrevs={},\n",
        "                 n_jobs=1):\n",
        "        \"\"\"\n",
        "        Text preprocessing transformer includes steps:\n",
        "            1. Text normalization\n",
        "            2. Punctuation removal\n",
        "            3. Stop words removal\n",
        "            4. Lemmatization\n",
        "\n",
        "        variety - format of date (AmE - american type, BrE - british format)\n",
        "        user_abbrevs - dict of user abbreviations mappings (from normalise package)\n",
        "        n_jobs - parallel jobs to run\n",
        "        \"\"\"\n",
        "        self.variety = variety\n",
        "        self.user_abbrevs = user_abbrevs\n",
        "        self.n_jobs = n_jobs\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, *_):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "        partitions = 1\n",
        "        cores = mp.cpu_count()\n",
        "        if self.n_jobs <= -1:\n",
        "            partitions = cores\n",
        "        elif self.n_jobs <= 0:\n",
        "            return X_copy.apply(self._preprocess_text)\n",
        "        else:\n",
        "            partitions = min(self.n_jobs, cores)\n",
        "\n",
        "        data_split = np.array_split(X_copy, partitions)\n",
        "        pool = mp.Pool(cores)\n",
        "        data = pd.concat(pool.map(self._preprocess_part, data_split))\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "\n",
        "        return data\n",
        "\n",
        "    def _preprocess_part(self, part):\n",
        "        return part.apply(self._preprocess_text)\n",
        "\n",
        "    def _preprocess_text(self, text):\n",
        "        normalized_text = self._normalize(text)\n",
        "        doc = nlp(normalized_text)\n",
        "        removed_punct = self._remove_punct(doc)\n",
        "        removed_stop_words = self._remove_stop_words(removed_punct)\n",
        "        return self._lemmatize(removed_stop_words)\n",
        "\n",
        "    def _normalize(self, text):\n",
        "        # some issues in normalise package\n",
        "        try:\n",
        "            return ' '.join(preprocessing.normalize(text, variety=self.variety, user_abbrevs=self.user_abbrevs, verbose=False))\n",
        "        except:\n",
        "            return text\n",
        "\n",
        "    def _remove_punct(self, doc):\n",
        "        return [t for t in doc if t.text not in string.punctuation]\n",
        "\n",
        "    def _remove_stop_words(self, doc):\n",
        "        return [t for t in doc if not t.is_stop]\n",
        "\n",
        "    def _lemmatize(self, doc):\n",
        "        return ' '.join([t.lemma_ for t in doc])"
      ],
      "metadata": {
        "id": "boriuNK6wiFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = list(data.columns.values)[5:]\n",
        "for column in columns:\n",
        "  data[column] = TextPreprocessor(n_jobs=-1).transform(data[column])"
      ],
      "metadata": {
        "id": "WPFllfkN2UVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(1)"
      ],
      "metadata": {
        "id": "8qFVjKYL2hxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "CxZXyKptcoYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes Model"
      ],
      "metadata": {
        "id": "Xyi19a9Qp50g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes model and evaluation\n",
        "def nb_model(data, column):\n",
        "  X, y = data[column], data['reviewerLocation']\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "  tvc_pipe = Pipeline([\n",
        "       ('tvec', TfidfVectorizer()),\n",
        "       ('mb', MultinomialNB())\n",
        "       ])\n",
        "\n",
        "  tvc_pipe.fit(X_train, y_train)\n",
        "\n",
        "  # Setting up grid search params\n",
        "  tf_params = {\n",
        "      'tvec__max_features':[100, 2000],\n",
        "      'tvec__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
        "      'tvec__stop_words': [None, 'english'],\n",
        "      }\n",
        "\n",
        "  # Fitting best parameters to the model\n",
        "  tvc_gs = GridSearchCV(tvc_pipe, param_grid=tf_params, cv = 5, verbose =0, n_jobs = -1)\n",
        "  tvc_gs.fit(X_train, y_train)\n",
        "\n",
        "  # Model Evaluation\n",
        "  tvc_gs_pred = tvc_gs.predict(X_test)\n",
        "  accuracy = accuracy_score(y_test, tvc_gs_pred)\n",
        "  f1 = f1_score(y_test, tvc_gs_pred, average='weighted')\n",
        "  precision = precision_score(y_test, tvc_gs_pred, average='weighted')\n",
        "  recall = recall_score(y_test, tvc_gs_pred, average='weighted')\n",
        "  metrics = [accuracy, f1, precision, recall]\n",
        "  return metrics, y_test, tvc_gs_pred"
      ],
      "metadata": {
        "id": "SRnoulUFvRvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "\n",
        "def run_nb_model(data, column):\n",
        "  # Run 5 times\n",
        "  metrics1, trueLabels1, predLabels1 = nb_model(data, column)\n",
        "  metrics2, trueLabels2, predLabels2 = nb_model(data, column)\n",
        "  metrics3, trueLabels3, predLabels3 = nb_model(data, column)\n",
        "  metrics4, trueLabels4, predLabels4 = nb_model(data, column)\n",
        "  metrics5, trueLabels5, predLabels5 = nb_model(data, column)\n",
        "\n",
        "  # Calculate mean and variance of each metric over each run\n",
        "  metricsList = [metrics1, metrics2, metrics3, metrics4, metrics5]\n",
        "  metricsMean=[]\n",
        "  metricsVariance=[]\n",
        "  for i in zip(*metricsList):\n",
        "    metricsMean.append(sum(i)/len(i))\n",
        "    metricsVariance.append(statistics.variance(i))\n",
        "\n",
        "  # Create dataframe of y_test/ predicted labels for each run\n",
        "  labels = pd.DataFrame(list(zip(trueLabels1, predLabels1, trueLabels2, predLabels2, trueLabels3, predLabels3, trueLabels4, predLabels4, trueLabels5, predLabels5,)),\n",
        "                        columns=['trueLabels1', 'predLabels1', 'trueLabels2', 'predLabels2', 'trueLabels3', 'predLabels3', 'trueLabels4', 'predLabels4', 'trueLabels5', 'predLabels5'])\n",
        "  return metricsMean, metricsVariance, labels"
      ],
      "metadata": {
        "id": "jBLMPDIvB5qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb = {}\n",
        "for column in columns:\n",
        "  mean = column + \"MetricsMean\"\n",
        "  var = column + \"MetricsVariance\"\n",
        "  lab = column + \"Labels\"\n",
        "  nb[mean], nb[var], nb[lab] = run_nb_model(data, column)\n",
        "  print(\"Column {} Complete!\".format(column))\n",
        "  print(nb[mean])"
      ],
      "metadata": {
        "id": "dmEQ890eNeRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Dictionary of Results\n",
        "np.save('nbDict.npy', nb)"
      ],
      "metadata": {
        "id": "6kniqTUBBLHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression Model"
      ],
      "metadata": {
        "id": "LFSHn-9creAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression model and evaluation\n",
        "def lr_model(data, column):\n",
        "  X, y = data[column], data['reviewerLocation']\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "  lr_pipe = Pipeline([\n",
        "       ('tvec', TfidfVectorizer()),\n",
        "       ('lr', LogisticRegression())\n",
        "       ])\n",
        "\n",
        "  lr_pipe.fit(X_train, y_train)\n",
        "\n",
        "  # Setting up grid search params\n",
        "  lr_params = {\n",
        "      'tvec__max_features':[2000],\n",
        "      'tvec__ngram_range': [(1, 2)],\n",
        "      'tvec__stop_words': ['english'],\n",
        "      'lr__penalty': ['l1','l2'],\n",
        "      'lr__C': [0.1, 1, 10, 100]\n",
        "      }\n",
        "\n",
        "  # Fitting best parameters to the model\n",
        "  lr_gs = GridSearchCV(lr_pipe, param_grid=lr_params, cv = 5, verbose = 0, n_jobs = -1)\n",
        "  lr_gs.fit(X_train, y_train)\n",
        "\n",
        "  # Model Evaluation\n",
        "  lr_gs_pred = lr_gs.predict(X_test)\n",
        "  accuracy = accuracy_score(y_test, lr_gs_pred)\n",
        "  f1 = f1_score(y_test, lr_gs_pred, average='weighted')\n",
        "  precision = precision_score(y_test, lr_gs_pred, average='weighted')\n",
        "  recall = recall_score(y_test, lr_gs_pred, average='weighted')\n",
        "  metrics = [accuracy, f1, precision, recall]\n",
        "  return metrics, y_test, lr_gs_pred"
      ],
      "metadata": {
        "id": "dEzphJFKreAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "\n",
        "def run_lr_model(data, column):\n",
        "  # Run 5 times\n",
        "  metrics1, trueLabels1, predLabels1 = lr_model(data, column)\n",
        "  metrics2, trueLabels2, predLabels2 = lr_model(data, column)\n",
        "  metrics3, trueLabels3, predLabels3 = lr_model(data, column)\n",
        "  metrics4, trueLabels4, predLabels4 = lr_model(data, column)\n",
        "  metrics5, trueLabels5, predLabels5 = lr_model(data, column)\n",
        "\n",
        "  # Calculate mean and variance of each metric over each run\n",
        "  metricsList = [metrics1, metrics2, metrics3, metrics4, metrics5]\n",
        "  metricsMean=[]\n",
        "  metricsVariance=[]\n",
        "  for i in zip(*metricsList):\n",
        "    metricsMean.append(sum(i)/len(i))\n",
        "    metricsVariance.append(statistics.variance(i))\n",
        "\n",
        "  # Create dataframe of y_test/ predicted labels for each run\n",
        "  labels = pd.DataFrame(list(zip(trueLabels1, predLabels1, trueLabels2, predLabels2, trueLabels3, predLabels3, trueLabels4, predLabels4, trueLabels5, predLabels5,)),\n",
        "                        columns=['trueLabels1', 'predLabels1', 'trueLabels2', 'predLabels2', 'trueLabels3', 'predLabels3', 'trueLabels4', 'predLabels4', 'trueLabels5', 'predLabels5'])\n",
        "  return metricsMean, metricsVariance, labels"
      ],
      "metadata": {
        "id": "RMRbWq5-reAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = {}\n",
        "for column in columns:\n",
        "  mean = column + \"MetricsMean\"\n",
        "  var = column + \"MetricsVariance\"\n",
        "  lab = column + \"Labels\"\n",
        "  lr[mean], lr[var], lr[lab] = run_lr_model(data, column)\n",
        "  print(\"Column {} Complete!\".format(column))\n",
        "  print(lr[mean])"
      ],
      "metadata": {
        "id": "CAD0G5_UreAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Dictionary of Results\n",
        "np.save('lrDict.npy', lr)"
      ],
      "metadata": {
        "id": "v_a0DmIsGFlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Machine Model"
      ],
      "metadata": {
        "id": "kMv8sU8voUUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest model and evaluation\n",
        "def svm_model(data, column):\n",
        "  X, y = data[column], data['reviewerLocation']\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "  svm_pipe = Pipeline([\n",
        "       ('tvec', TfidfVectorizer()),\n",
        "       ('svm', SVC())\n",
        "       ])\n",
        "\n",
        "  svm_pipe.fit(X_train, y_train)\n",
        "\n",
        "  # Setting up grid search params\n",
        "  svm_params = {\n",
        "      'tvec__max_features':[2000],\n",
        "      'tvec__ngram_range': [(1, 2)],\n",
        "      'tvec__stop_words': ['english'],\n",
        "      'svm__C': [0.1, 1, 10, 100],\n",
        "      'svm__gamma': [1, 0.1, 0.01, 0.001],\n",
        "      'svm__kernel': ['rbf']\n",
        "      }\n",
        "\n",
        "  # Fitting best parameters to the model\n",
        "  svm_gs = GridSearchCV(svm_pipe, param_grid=svm_params, cv = 5, verbose = 0, n_jobs = -1)\n",
        "  svm_gs.fit(X_train, y_train)\n",
        "\n",
        "  # Model Evaluation\n",
        "  svm_gs_pred = svm_gs.predict(X_test)\n",
        "  accuracy = accuracy_score(y_test, svm_gs_pred)\n",
        "  f1 = f1_score(y_test, svm_gs_pred, average='weighted')\n",
        "  precision = precision_score(y_test, svm_gs_pred, average='weighted')\n",
        "  recall = recall_score(y_test, svm_gs_pred, average='weighted')\n",
        "  metrics = [accuracy, f1, precision, recall]\n",
        "  return metrics, y_test, svm_gs_pred"
      ],
      "metadata": {
        "id": "4mIRTGa_oUUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "\n",
        "def run_svm_model(data, column):\n",
        "  # Run 5 times\n",
        "  metrics1, trueLabels1, predLabels1 = svm_model(data, column)\n",
        "  metrics2, trueLabels2, predLabels2 = svm_model(data, column)\n",
        "  metrics3, trueLabels3, predLabels3 = svm_model(data, column)\n",
        "  metrics4, trueLabels4, predLabels4 = svm_model(data, column)\n",
        "  metrics5, trueLabels5, predLabels5 = svm_model(data, column)\n",
        "\n",
        "  # Calculate mean and variance of each metric over each run\n",
        "  metricsList = [metrics1, metrics2, metrics3, metrics4, metrics5]\n",
        "  metricsMean=[]\n",
        "  metricsVariance=[]\n",
        "  for i in zip(*metricsList):\n",
        "    metricsMean.append(sum(i)/len(i))\n",
        "    metricsVariance.append(statistics.variance(i))\n",
        "\n",
        "  # Create dataframe of y_test/ predicted labels for each run\n",
        "  labels = pd.DataFrame(list(zip(trueLabels1, predLabels1, trueLabels2, predLabels2, trueLabels3, predLabels3, trueLabels4, predLabels4, trueLabels5, predLabels5,)),\n",
        "                        columns=['trueLabels1', 'predLabels1', 'trueLabels2', 'predLabels2', 'trueLabels3', 'predLabels3', 'trueLabels4', 'predLabels4', 'trueLabels5', 'predLabels5'])\n",
        "  return metricsMean, metricsVariance, labels"
      ],
      "metadata": {
        "id": "n56d3CVUoUUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm = {}\n",
        "for column in columns:\n",
        "  mean = column + \"MetricsMean\"\n",
        "  var = column + \"MetricsVariance\"\n",
        "  lab = column + \"Labels\"\n",
        "  svm[mean], svm[var], svm[lab] = run_svm_model(data, column)\n",
        "  print(\"Column {} Complete!\".format(column))\n",
        "  print(svm[mean])"
      ],
      "metadata": {
        "id": "RlLDYGXGoUUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Dictionary of Results\n",
        "np.save('svmDict.npy', svm)"
      ],
      "metadata": {
        "id": "3t6kiK-jGWlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Model"
      ],
      "metadata": {
        "id": "x4j55mt0jZSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest model and evaluation\n",
        "def rf_model(data, column):\n",
        "  X, y = data[column], data['reviewerLocation']\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "  rf_pipe = Pipeline([\n",
        "       ('tvec', TfidfVectorizer()),\n",
        "       ('rf', RandomForestClassifier())\n",
        "       ])\n",
        "\n",
        "  rf_pipe.fit(X_train, y_train)\n",
        "\n",
        "  # Setting up grid search params\n",
        "  rf_params = {\n",
        "      'tvec__max_features':[2000],\n",
        "      'tvec__ngram_range': [(1, 2)],\n",
        "      'tvec__stop_words': ['english'],\n",
        "      'rf__max_depth': [1000],\n",
        "      'rf__min_samples_split': [100],\n",
        "      'rf__max_leaf_nodes': [None]\n",
        "      }\n",
        "\n",
        "  # Fitting best parameters to the model\n",
        "  rf_gs = GridSearchCV(rf_pipe, param_grid=rf_params, cv = 5, verbose = 0, n_jobs = -1)\n",
        "  rf_gs.fit(X_train, y_train)\n",
        "\n",
        "  # Model Evaluation\n",
        "  rf_gs_pred = rf_gs.predict(X_test)\n",
        "  accuracy = accuracy_score(y_test, rf_gs_pred)\n",
        "  f1 = f1_score(y_test, rf_gs_pred, average='weighted')\n",
        "  precision = precision_score(y_test, rf_gs_pred, average='weighted')\n",
        "  recall = recall_score(y_test, rf_gs_pred, average='weighted')\n",
        "  metrics = [accuracy, f1, precision, recall]\n",
        "  return metrics, y_test, rf_gs_pred"
      ],
      "metadata": {
        "id": "ymE6XskyjZSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "\n",
        "def run_rf_model(data, column):\n",
        "  # Run 5 times\n",
        "  metrics1, trueLabels1, predLabels1 = rf_model(data, column)\n",
        "  metrics2, trueLabels2, predLabels2 = rf_model(data, column)\n",
        "  metrics3, trueLabels3, predLabels3 = rf_model(data, column)\n",
        "  metrics4, trueLabels4, predLabels4 = rf_model(data, column)\n",
        "  metrics5, trueLabels5, predLabels5 = rf_model(data, column)\n",
        "\n",
        "  # Calculate mean and variance of each metric over each run\n",
        "  metricsList = [metrics1, metrics2, metrics3, metrics4, metrics5]\n",
        "  metricsMean=[]\n",
        "  metricsVariance=[]\n",
        "  for i in zip(*metricsList):\n",
        "    metricsMean.append(sum(i)/len(i))\n",
        "    metricsVariance.append(statistics.variance(i))\n",
        "\n",
        "  # Create dataframe of y_test/ predicted labels for each run\n",
        "  labels = pd.DataFrame(list(zip(trueLabels1, predLabels1, trueLabels2, predLabels2, trueLabels3, predLabels3, trueLabels4, predLabels4, trueLabels5, predLabels5,)),\n",
        "                        columns=['trueLabels1', 'predLabels1', 'trueLabels2', 'predLabels2', 'trueLabels3', 'predLabels3', 'trueLabels4', 'predLabels4', 'trueLabels5', 'predLabels5'])\n",
        "  return metricsMean, metricsVariance, labels"
      ],
      "metadata": {
        "id": "66eRo6FWjZSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = {}\n",
        "for column in columns:\n",
        "  mean = column + \"MetricsMean\"\n",
        "  var = column + \"MetricsVariance\"\n",
        "  lab = column + \"Labels\"\n",
        "  rf[mean], rf[var], rf[lab] = run_rf_model(data, column)\n",
        "  print(\"Column {} Complete!\".format(column))\n",
        "  print(rf[mean])"
      ],
      "metadata": {
        "id": "OHK-FqmejZSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Dictionary of Results\n",
        "np.save('rfDict.npy', rf)"
      ],
      "metadata": {
        "id": "HDyGtdDXGRld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting Model"
      ],
      "metadata": {
        "id": "welFsfGEamdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosting model and evaluation\n",
        "def xg_model(data, column):\n",
        "  X, y = data[column], data['reviewerLocation']\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "  xg_pipe = Pipeline([\n",
        "       ('tvec', TfidfVectorizer()),\n",
        "       ('xg', XGBClassifier())\n",
        "       ])\n",
        "\n",
        "  xg_pipe.fit(X_train, y_train)\n",
        "\n",
        "  # Setting up grid search params\n",
        "  xg_params = {\n",
        "      'tvec__max_features':[2000],\n",
        "      'tvec__ngram_range': [(1, 2)],\n",
        "      'tvec__stop_words': ['english'],\n",
        "      'xg__n_estimators': [50, 100],\n",
        "      'xg__learning_rate': [0.1]\n",
        "      }\n",
        "\n",
        "  # Fitting best parameters to the model\n",
        "  xg_gs = GridSearchCV(xg_pipe, param_grid=xg_params, cv = 5, verbose = 0, n_jobs = -1)\n",
        "  xg_gs.fit(X_train, y_train)\n",
        "\n",
        "  # Model Evaluation\n",
        "  xg_gs_pred = xg_gs.predict(X_test)\n",
        "  accuracy = accuracy_score(y_test, xg_gs_pred)\n",
        "  f1 = f1_score(y_test, xg_gs_pred, average='weighted')\n",
        "  precision = precision_score(y_test, xg_gs_pred, average='weighted')\n",
        "  recall = recall_score(y_test, xg_gs_pred, average='weighted')\n",
        "  metrics = [accuracy, f1, precision, recall]\n",
        "  return metrics, y_test, xg_gs_pred"
      ],
      "metadata": {
        "id": "5QE8Ff1MamdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "\n",
        "def run_xg_model(data, column):\n",
        "  # Run 5 times\n",
        "  metrics1, trueLabels1, predLabels1 = xg_model(data, column)\n",
        "  metrics2, trueLabels2, predLabels2 = xg_model(data, column)\n",
        "  metrics3, trueLabels3, predLabels3 = xg_model(data, column)\n",
        "  metrics4, trueLabels4, predLabels4 = xg_model(data, column)\n",
        "  metrics5, trueLabels5, predLabels5 = xg_model(data, column)\n",
        "\n",
        "  # Calculate mean and variance of each metric over each run\n",
        "  metricsList = [metrics1, metrics2, metrics3, metrics4, metrics5]\n",
        "  metricsMean=[]\n",
        "  metricsVariance=[]\n",
        "  for i in zip(*metricsList):\n",
        "    metricsMean.append(sum(i)/len(i))\n",
        "    metricsVariance.append(statistics.variance(i))\n",
        "\n",
        "  # Create dataframe of y_test/ predicted labels for each run\n",
        "  labels = pd.DataFrame(list(zip(trueLabels1, predLabels1, trueLabels2, predLabels2, trueLabels3, predLabels3, trueLabels4, predLabels4, trueLabels5, predLabels5,)),\n",
        "                        columns=['trueLabels1', 'predLabels1', 'trueLabels2', 'predLabels2', 'trueLabels3', 'predLabels3', 'trueLabels4', 'predLabels4', 'trueLabels5', 'predLabels5'])\n",
        "  return metricsMean, metricsVariance, labels"
      ],
      "metadata": {
        "id": "aljLBlbqamdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xg = {}\n",
        "for column in columns:\n",
        "  mean = column + \"MetricsMean\"\n",
        "  var = column + \"MetricsVariance\"\n",
        "  lab = column + \"Labels\"\n",
        "  xg[mean], xg[var], xg[lab] = run_xg_model(data, column)\n",
        "  print(\"Column {} Complete!\".format(column))\n",
        "  print(xg[mean])"
      ],
      "metadata": {
        "id": "Qba5-tA1amdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Dictionary of Results\n",
        "np.save('xgDict.npy', xg)"
      ],
      "metadata": {
        "id": "bXYR08sttgOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Importance"
      ],
      "metadata": {
        "id": "sreduE9IjMS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick fit of pipelines outside of def for both original and summaries\n",
        "X, y = data['originalReview'], data['reviewerLocation']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=64)\n",
        "\n",
        "tvc_or_pipe = Pipeline([\n",
        "     ('tvec', TfidfVectorizer()),\n",
        "     ('mb', MultinomialNB())\n",
        "     ])\n",
        "tvc_or_pipe.fit(X_train, y_train)\n",
        "\n",
        "rf_or_pipe = Pipeline([\n",
        "     ('tvec', TfidfVectorizer()),\n",
        "     ('rf', RandomForestClassifier())\n",
        "     ])\n",
        "rf_or_pipe.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "dp1k2VNx0fYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = data['summary'], data['reviewerLocation']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=64)\n",
        "\n",
        "tvc_su_pipe = Pipeline([\n",
        "     ('tvec', TfidfVectorizer()),\n",
        "     ('mb', MultinomialNB())\n",
        "     ])\n",
        "tvc_su_pipe.fit(X_train, y_train)\n",
        "\n",
        "rf_su_pipe = Pipeline([\n",
        "     ('tvec', TfidfVectorizer()),\n",
        "     ('rf', RandomForestClassifier())\n",
        "     ])\n",
        "rf_su_pipe.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "jdwRwd9p5ojP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_title = pd.DataFrame(rf_or_pipe.steps[1][1].feature_importances_, tvc_or_pipe.steps[0][1].get_feature_names_out(), columns=['originalImportance'])\n",
        "original_title = original_title.sort_values('originalImportance', ascending = False).head(20)\n",
        "original_title = original_title.reset_index()\n",
        "original_title = original_title.rename(columns={\"index\": \"originalWord\"})\n",
        "\n",
        "summary_title = pd.DataFrame(rf_su_pipe.steps[1][1].feature_importances_, tvc_su_pipe.steps[0][1].get_feature_names_out(), columns=['summaryImportance'])\n",
        "summary_title = summary_title.sort_values('summaryImportance', ascending = False).head(20)\n",
        "summary_title = summary_title.reset_index()\n",
        "summary_title = summary_title.rename(columns={\"index\": \"summaryWord\"})\n",
        "\n",
        "result = pd.concat([original_title, summary_title], axis=1)\n",
        "result"
      ],
      "metadata": {
        "id": "hdSwnSnKnsel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 1, figsize=(8, 9), constrained_layout=True, sharex=True)\n",
        "fig.suptitle('Word Importance Comparison')\n",
        "\n",
        "sns.barplot(x=result['originalImportance'], y=result['originalWord'], palette='Paired', ax=axes[0])\n",
        "axes[0].set_title('Top 20 Most Important Words For Classification of Original Reviews')\n",
        "\n",
        "sns.barplot(x=result['summaryImportance'], y=result['summaryWord'], palette='Paired', ax=axes[1])\n",
        "axes[1].set_title('Top 20 Most Important Words For Classification of Summary Reviews')"
      ],
      "metadata": {
        "id": "mZO3W6vT-NKS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}