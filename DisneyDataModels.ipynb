{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO4Zo6+UxLtk8c5Fys8aklM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Inits"],"metadata":{"id":"oi3hlbTTe4TH"}},{"cell_type":"code","source":["!pip install xgboost tqdm tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4YW-zJf7eJFJ","executionInfo":{"status":"ok","timestamp":1747902182190,"user_tz":-60,"elapsed":3349,"user":{"displayName":"Charmaine Barker","userId":"16230026927320722913"}},"outputId":"4dddc143-7e30-41a4-8fa4-5db298b0c238"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"]}]},{"cell_type":"code","source":["!rm glove.6B.zip*\n","!wget -O glove.6B.zip https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","!unzip glove.6B.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"01D62SpJZN7x","executionInfo":{"status":"ok","timestamp":1747903168846,"user_tz":-60,"elapsed":197488,"user":{"displayName":"Charmaine Barker","userId":"16230026927320722913"}},"outputId":"80e73ae9-38c4-4063-eb39-079cf43b3d77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-05-22 08:36:11--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip        100%[===================>] 822.24M  4.46MB/s    in 2m 39s  \n","\n","2025-05-22 08:38:50 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n","\n","Archive:  glove.6B.zip\n","  inflating: glove.6B.50d.txt        \n","  inflating: glove.6B.100d.txt       \n","  inflating: glove.6B.200d.txt       \n","  inflating: glove.6B.300d.txt       \n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Activation, Bidirectional, BatchNormalization\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n","from tensorflow.keras.optimizers import AdamW\n","from sklearn.preprocessing import LabelBinarizer\n","\n","from xgboost import XGBClassifier\n","import tempfile\n","import re\n","from tqdm import tqdm, trange\n","import os\n","from nltk.corpus import stopwords, wordnet\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","import pickle"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FJ9XHX1Jd_t0","executionInfo":{"status":"ok","timestamp":1747915196501,"user_tz":-60,"elapsed":30,"user":{"displayName":"Charmaine Barker","userId":"16230026927320722913"}},"outputId":"d8b9a95e-5cd3-49d7-f028-e4a2bf69f69c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["## Classes"],"metadata":{"id":"nG3nW0Lle6N7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"N3JLIHtbdtP8"},"outputs":[],"source":["class TextPreprocessor:\n","    def __init__(self, text_column):\n","        self.text_column = text_column\n","        self.stop_words = set(stopwords.words('english'))\n","        self.lemmatizer = WordNetLemmatizer()\n","\n","    def clean_text(self, text):\n","        text = text.lower()\n","        text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n","        tokens = text.split()\n","        tokens = [word for word in tokens if word not in self.stop_words]\n","        tokens = [self.lemmatizer.lemmatize(word) for word in tokens]\n","        return \" \".join(tokens)\n","\n","    def process(self, df):\n","        df = df.copy()\n","        df[self.text_column] = df[self.text_column].astype(str).apply(self.clean_text)\n","        return df\n","\n","class XGBoostTFIDFClassifier:\n","    def __init__(self):\n","        self.vectorizer = TfidfVectorizer()\n","        self.model = XGBClassifier(eval_metric='logloss')\n","\n","    def fit(self, X_train, y_train):\n","        X_tfidf = self.vectorizer.fit_transform(X_train)\n","        self.model.fit(X_tfidf, y_train)\n","\n","    def predict(self, X_test):\n","        X_tfidf = self.vectorizer.transform(X_test)\n","        return self.model.predict(X_tfidf)\n","\n","class LSTMEmbeddingClassifier:\n","    def __init__(self, max_words=10000, max_len=200, embed_dim=200, glove_path='glove.6B.200d.txt'):\n","        self.max_words = max_words\n","        self.max_len = max_len\n","        self.embed_dim = embed_dim\n","        self.glove_path = glove_path\n","        self.tokenizer = Tokenizer(num_words=self.max_words, oov_token=\"<OOV>\")\n","        self.model = None\n","        self.lb = LabelBinarizer()\n","        self.embedding_matrix = None\n","\n","    def load_glove_embeddings(self):\n","        embeddings_index = {}\n","        with open(self.glove_path, encoding='utf8') as f:\n","            for line in f:\n","                values = line.split()\n","                word = values[0]\n","                coeffs = np.asarray(values[1:], dtype='float32')\n","                embeddings_index[word] = coeffs\n","\n","        word_index = self.tokenizer.word_index\n","        embedding_matrix = np.zeros((self.max_words, self.embed_dim))\n","        for word, i in word_index.items():\n","            if i >= self.max_words:\n","                continue\n","            embedding_vector = embeddings_index.get(word)\n","            if embedding_vector is not None:\n","                embedding_matrix[i] = embedding_vector\n","\n","        self.embedding_matrix = embedding_matrix\n","\n","    def fit(self, X_train, y_train):\n","        self.tokenizer.fit_on_texts(X_train)\n","        X_seq = self.tokenizer.texts_to_sequences(X_train)\n","        X_pad = pad_sequences(X_seq, maxlen=self.max_len)\n","        y_encoded = self.lb.fit_transform(y_train)\n","\n","        self.load_glove_embeddings()\n","\n","        self.model = Sequential([\n","            Embedding(input_dim=self.max_words, output_dim=self.embed_dim,\n","                      weights=[self.embedding_matrix], trainable=True),\n","            Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout=0.2)),\n","            Dropout(0.3),\n","            Bidirectional(LSTM(32, return_sequences=False, recurrent_dropout=0.2)),\n","            Dropout(0.5),\n","            Dense(64),\n","            BatchNormalization(),\n","            Activation('relu'),\n","            Dropout(0.5),\n","            Dense(len(self.lb.classes_), activation='softmax')\n","        ])\n","\n","        optimizer = AdamW(learning_rate=3e-4, weight_decay=5e-4)\n","        self.model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","        self.model.fit(\n","            X_pad, y_encoded,\n","            epochs=50,\n","            batch_size=64,\n","            validation_split=0.2,\n","            callbacks=[\n","                EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True),\n","                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n","            ],\n","            verbose=1\n","        )\n","\n","    def predict(self, X_test):\n","        X_seq = self.tokenizer.texts_to_sequences(X_test)\n","        X_pad = pad_sequences(X_seq, maxlen=self.max_len)\n","        preds = self.model.predict(X_pad, verbose=0)\n","        y_pred = self.lb.inverse_transform(preds)\n","        return y_pred\n","\n","class TextClassifierFramework:\n","    def __init__(self, model_type='xgboost'):\n","        self.model_type = model_type\n","        self.model = None\n","        self.text_column = None\n","        self.target_column = None\n","        self.preprocessor = None\n","        self.label_encoder = None\n","        self.y_train_str = None\n","        self.y_test_str = None\n","\n","    def load_data(self, df, text_column, target_column, test_size=0.2, random_state=None):\n","        self.text_column = text_column\n","        self.target_column = target_column\n","        self.preprocessor = TextPreprocessor(text_column)\n","        self.label_encoder = LabelEncoder()\n","\n","        df = self.preprocessor.process(df)\n","\n","        X_train, X_test, y_train_str, y_test_str = train_test_split(\n","            df[text_column], df[target_column],\n","            test_size=test_size, random_state=random_state, stratify=df[target_column]\n","        )\n","\n","        self.y_train_str = y_train_str.reset_index(drop=True)\n","        self.y_test_str = y_test_str.reset_index(drop=True)\n","\n","        y_train = self.label_encoder.fit_transform(self.y_train_str)\n","        y_test = self.label_encoder.transform(self.y_test_str)\n","\n","        if self.model_type == 'xgboost':\n","            self.model = XGBoostTFIDFClassifier()\n","        elif self.model_type == 'lstm':\n","            self.model = LSTMEmbeddingClassifier()\n","        else:\n","            raise ValueError(\"Unsupported model type\")\n","\n","        return X_train.reset_index(drop=True), X_test.reset_index(drop=True), y_train, y_test\n","\n","    def train(self, X_train, y_train):\n","        self.model.fit(X_train, y_train)\n","\n","    def evaluate(self, X_test, y_test):\n","        preds = self.model.predict(X_test)\n","        if isinstance(preds[0], str):\n","            y_test_str = self.y_test_str\n","            preds_str = preds\n","        else:\n","            y_test_str = self.label_encoder.inverse_transform(y_test)\n","            preds_str = self.label_encoder.inverse_transform(preds)\n","\n","        acc = accuracy_score(y_test_str, preds_str)\n","        precision, recall, f1, _ = precision_recall_fscore_support(y_test_str, preds_str, average='macro', zero_division=0)\n","        report = classification_report(y_test_str, preds_str, output_dict=True, zero_division=0)\n","        return {\n","            'accuracy': acc,\n","            'precision': precision,\n","            'recall': recall,\n","            'f1': f1,\n","            'classification_report': report,\n","            'y_true': list(y_test_str),\n","            'y_pred': list(preds_str)\n","        }\n","\n","class RepeatedEvaluator:\n","    def __init__(self, df, text_column, target_column, model_type='xgboost', runs=10):\n","        self.df = df\n","        self.text_column = text_column\n","        self.target_column = target_column\n","        self.model_type = model_type\n","        self.runs = runs\n","        self.results = []\n","\n","    def run(self):\n","        for run in range(self.runs):\n","            print(f\"Run {run + 1}/{self.runs}\")\n","            clf = TextClassifierFramework(model_type=self.model_type)\n","            X_train, X_test, y_train, y_test = clf.load_data(\n","                self.df, self.text_column, self.target_column, random_state=run\n","            )\n","            clf.train(X_train, y_train)\n","            eval_result = clf.evaluate(X_test, y_test)\n","            self.results.append(eval_result)\n","        return self.results"]},{"cell_type":"markdown","source":["## Import Dataset"],"metadata":{"id":"oEVwdmKUe8cN"}},{"cell_type":"code","source":["shared_link = '*insert file sharing link from your local google drive, or import the csv another way*'\n","file_id = re.search(r'/d/(.*?)/', shared_link).group(1)\n","download_url = f'https://drive.google.com/uc?id={file_id}'\n","df = pd.read_csv(download_url)"],"metadata":{"id":"cbuVlLMje-YC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Running"],"metadata":{"id":"oAbbJsPFfiTu"}},{"cell_type":"code","source":["evaluator = RepeatedEvaluator(\n","    df=df,\n","    text_column='rawText',\n","    target_column='Reviewer_Location',\n","    model_type='xgboost',  # xgboost or lstm\n","    runs=1\n",")\n","\n","results = evaluator.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pikwrudjflwy","executionInfo":{"status":"ok","timestamp":1747927359671,"user_tz":-60,"elapsed":164670,"user":{"displayName":"Charmaine Barker","userId":"16230026927320722913"}},"outputId":"1a24da37-d7cf-4de2-aff5-54dbc90aeb8c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Run 1/1\n"]}]},{"cell_type":"markdown","source":["## Results Analysis"],"metadata":{"id":"HzwSnCHC4wTh"}},{"cell_type":"code","source":["accuracies = [r['accuracy'] for r in results]\n","print(\"Mean Accuracy:\", np.mean(accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UfUPDrZZ4zEC","executionInfo":{"status":"ok","timestamp":1747927363237,"user_tz":-60,"elapsed":31,"user":{"displayName":"Charmaine Barker","userId":"16230026927320722913"}},"outputId":"e8241e1b-a264-451c-e0db-4bfa392eb20d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Accuracy: 0.5055\n"]}]},{"cell_type":"code","source":["#with open(\"raw_xgboost.pkl\", \"wb\") as f:\n","#    pickle.dump(results, f)"],"metadata":{"id":"L63SS2I2FW8H"},"execution_count":null,"outputs":[]}]}