{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Using the Disneyland Dataset available from kaggle at:\n",
        "https://www.kaggle.com/datasets/arushchillar/disneyland-reviews"
      ],
      "metadata": {
        "id": "IqxRGSzO9Wrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from mlxtend.evaluate import mcnemar_table"
      ],
      "metadata": {
        "id": "MJxP4Bt6-vyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "o5flplXh_3x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dictionaries\n",
        "nbDict = np.load('/content/drive/MyDrive/MyPapers/ChatPaper/Version2/Code Data/ResultsDictionaries/OneRun/nbDict.npy', allow_pickle='TRUE').item()\n",
        "lrDict = np.load('/content/drive/MyDrive/MyPapers/ChatPaper/Version2/Code Data/ResultsDictionaries/OneRun/lrDict.npy', allow_pickle='TRUE').item()\n",
        "svmDict = np.load('/content/drive/MyDrive/MyPapers/ChatPaper/Version2/Code Data/ResultsDictionaries/OneRun/svmDict.npy', allow_pickle='TRUE').item()\n",
        "rfDict = np.load('/content/drive/MyDrive/MyPapers/ChatPaper/Version2/Code Data/ResultsDictionaries/OneRun/rfDict.npy', allow_pickle='TRUE').item()\n",
        "xgDict = np.load('/content/drive/MyDrive/MyPapers/ChatPaper/Version2/Code Data/ResultsDictionaries/OneRun/xgDict.npy', allow_pickle='TRUE').item()"
      ],
      "metadata": {
        "id": "6AgkqLFdBlZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes\n",
        "columns = ['summary','formal','informal','americanEnglish','britishEnglish',\n",
        "           'australianEnglish','yorkshire','factual']\n",
        "ignore={}\n",
        "\n",
        "nbOriginalLabels = nbDict['originalReviewLabels']\n",
        "y_true = nbOriginalLabels['trueLabels']\n",
        "y_original = nbOriginalLabels['predLabels']\n",
        "\n",
        "# H0 = Model 1 and 2 have the same error rate, no statistically significant difference\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "threshold = 3.841\n",
        "significance_value = 0.05\n",
        "\n",
        "for column in columns:\n",
        "  print('The {} model vs the original review model'.format(column))\n",
        "  lab = column + \"Labels\"\n",
        "  comparisonLabels = nbDict[lab]\n",
        "  y_summary = comparisonLabels['predLabels']\n",
        "\n",
        "  nbtable = mcnemar_table(y_target=y_true,\n",
        "                      y_model1=y_original,\n",
        "                      y_model2=y_summary)\n",
        "\n",
        "  # McNemar's Test with the continuity correction\n",
        "  test = mcnemar(nbtable, exact=False, correction=True)\n",
        "  if test.pvalue < significance_value:\n",
        "    print(\"Reject Null hypothesis\")\n",
        "  else:\n",
        "    print(\"Fail to reject Null hypothesis\")\n",
        "\n",
        "  #or equivalently\n",
        "  if test.statistic > threshold:\n",
        "    print(\"Reject Null hypothesis\")\n",
        "  else:\n",
        "    print(\"Fail to reject Null hypothesis\")"
      ],
      "metadata": {
        "id": "h4tXYnZ2J8er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "columns = ['summary','formal','informal','americanEnglish','britishEnglish',\n",
        "           'australianEnglish','yorkshire','factual']\n",
        "ignore={}\n",
        "\n",
        "lrOriginalLabels = lrDict['originalReviewLabels']\n",
        "y_true = lrOriginalLabels['trueLabels']\n",
        "y_original = lrOriginalLabels['predLabels']\n",
        "\n",
        "# H0 = Model 1 and 2 have the same error rate, no statistically significant difference\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "threshold = 3.841\n",
        "significance_value = 0.05\n",
        "\n",
        "for column in columns:\n",
        "  print('The {} model vs the original review model'.format(column))\n",
        "  lab = column + \"Labels\"\n",
        "  comparisonLabels = lrDict[lab]\n",
        "  y_summary = comparisonLabels['predLabels']\n",
        "\n",
        "  lrtable = mcnemar_table(y_target=y_true,\n",
        "                      y_model1=y_original,\n",
        "                      y_model2=y_summary)\n",
        "\n",
        "  # McNemar's Test with the continuity correction\n",
        "  test = mcnemar(lrtable, exact=False, correction=True)\n",
        "  if test.pvalue < significance_value:\n",
        "    print(\"Reject Null hypothesis\")\n",
        "  else:\n",
        "    print(\"Fail to reject Null hypothesis\")\n",
        "\n",
        "  #or equivalently\n",
        "  if test.statistic > threshold:\n",
        "    print(\"Reject Null hypothesis\")\n",
        "  else:\n",
        "    print(\"Fail to reject Null hypothesis\")"
      ],
      "metadata": {
        "id": "XUDVlGKxMEf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Support Vector Classification\n",
        "columns = ['summary','formal','informal','americanEnglish','britishEnglish',\n",
        "           'australianEnglish','yorkshire','factual']\n",
        "ignore={}\n",
        "\n",
        "svmOriginalLabels = svmDict['originalReviewLabels']\n",
        "y_true = svmOriginalLabels['trueLabels']\n",
        "y_original = svmOriginalLabels['predLabels']\n",
        "\n",
        "# H0 = Model 1 and 2 have the same error rate, no statistically significant difference\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "threshold = 3.841\n",
        "significance_value = 0.05\n",
        "\n",
        "for column in columns:\n",
        "  print('The {} model vs the original review model'.format(column))\n",
        "  lab = column + \"Labels\"\n",
        "  comparisonLabels = svmDict[lab]\n",
        "  y_summary = comparisonLabels['predLabels']\n",
        "\n",
        "  svmtable = mcnemar_table(y_target=y_true,\n",
        "                      y_model1=y_original,\n",
        "                      y_model2=y_summary)\n",
        "\n",
        "  # McNemar's Test with the continuity correction\n",
        "  test = mcnemar(svmtable, exact=False, correction=True)\n",
        "  if test.pvalue < significance_value:\n",
        "    print(\"Reject Null hypothesis\")\n",
        "  else:\n",
        "    print(\"Fail to reject Null hypothesis\")\n",
        "\n",
        "  #or equivalently\n",
        "  if test.statistic > threshold:\n",
        "    print(\"Reject Null hypothesis\")\n",
        "  else:\n",
        "    print(\"Fail to reject Null hypothesis\")"
      ],
      "metadata": {
        "id": "cZ1e9EQSMFut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "columns = ['summary','formal','informal','americanEnglish','britishEnglish',\n",
        "           'australianEnglish','yorkshire','factual']\n",
        "ignore={}\n",
        "\n",
        "rfOriginalLabels = rfDict['originalReviewLabels']\n",
        "y_true = rfOriginalLabels['trueLabels']\n",
        "y_original = rfOriginalLabels['predLabels']\n",
        "\n",
        "# H0 = Model 1 and 2 have the same error rate, no statistically significant difference\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "threshold = 3.841\n",
        "significance_value = 0.05\n",
        "\n",
        "for column in columns:\n",
        "  print('The {} model vs the original review model'.format(column))\n",
        "  lab = column + \"Labels\"\n",
        "  comparisonLabels = rfDict[lab]\n",
        "  y_summary = comparisonLabels['predLabels']\n",
        "\n",
        "  rftable = mcnemar_table(y_target=y_true,\n",
        "                      y_model1=y_original,\n",
        "                      y_model2=y_summary)\n",
        "\n",
        "  # McNemar's Test with the continuity correction\n",
        "  test = mcnemar(rftable, exact=False, correction=True)\n",
        "  if test.pvalue < significance_value:\n",
        "    print(\"Reject Null hypothesis\")\n",
        "  else:\n",
        "    print(\"Fail to reject Null hypothesis\")\n",
        "\n",
        "  #or equivalently\n",
        "  if test.statistic > threshold:\n",
        "    print(\"Reject Null hypothesis\")\n",
        "  else:\n",
        "    print(\"Fail to reject Null hypothesis\")"
      ],
      "metadata": {
        "id": "aS1nZ345MGWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost\n",
        "columns = ['summary','formal','informal','americanEnglish','britishEnglish',\n",
        "           'australianEnglish','yorkshire','factual']\n",
        "ignore={}\n",
        "\n",
        "xgOriginalLabels = xgDict['originalReviewLabels']\n",
        "y_true = xgOriginalLabels['trueLabels']\n",
        "y_original = xgOriginalLabels['predLabels']\n",
        "\n",
        "# H0 = Model 1 and 2 have the same error rate, no statistically significant difference\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "threshold = 3.841\n",
        "significance_value = 0.05\n",
        "\n",
        "for column in columns:\n",
        "  print('The {} model vs the original review model'.format(column))\n",
        "  lab = column + \"Labels\"\n",
        "  comparisonLabels = xgDict[lab]\n",
        "  y_summary = comparisonLabels['predLabels']\n",
        "\n",
        "  xgtable = mcnemar_table(y_target=y_true,\n",
        "                      y_model1=y_original,\n",
        "                      y_model2=y_summary)\n",
        "\n",
        "  # McNemar's Test with the continuity correction\n",
        "  test = mcnemar(xgtable, exact=False, correction=True)\n",
        "  if test.pvalue < significance_value:\n",
        "    print(\"Reject Null hypothesis\")\n",
        "  else:\n",
        "    print(\"Fail to reject Null hypothesis\")\n",
        "\n",
        "  #or equivalently\n",
        "  if test.statistic > threshold:\n",
        "    print(\"Reject Null hypothesis\")\n",
        "  else:\n",
        "    print(\"Fail to reject Null hypothesis\")"
      ],
      "metadata": {
        "id": "69Avv3kWMGyA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}